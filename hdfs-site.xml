<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/usr/local/hadoop/hdfs/namenode</value>
 <description>Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table
is replicated in all of the directories, for redundancy.
 </description>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>/usr/local/hadoop/hdfs/datanode</value>
 <description>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all n
amed directories, typically on different devices. Directories that do not exist are ignored.
 </description>
</property>
<property>
   <name>dfs.namenode.secondary.http-address</name>
   <value>node0:50090</value>
</property>
<property>
   <name>dfs.namenode.checkpoint.dir</name>
   <value>file:///usr/local/hadoop/hdfs/namesecondary</value>
</property>
<!--指定hdfs的nameservice为bigdatacluster-ha，需要和core-site.xml中的保持一致 -->   
    <property>   
        <name>dfs.nameservices</name>   
        <value>node0</value>   
    </property>  
<!--指定磁盘预留多少空间，防止磁盘被撑满用完，单位为bytes -->   
    <property>  
        <name>dfs.datanode.du.reserved</name>  
        <value>1024000000</value>  
    </property>  
    <!-- bigdatacluster-ha下面有两个NameNode，分别是namenode1，namenode2 -->   
    <property>   
        <name>dfs.ha.namenodes.node0</name>   
        <value>namenode1,namenode2</value>   
    </property>   
    <!-- namenode1的RPC通信地址,这里端口要和core-site.xml中fs.defaultFS保持一致 -->   
    <property>   
        <name>dfs.namenode.rpc-address.node0.namenode1</name>   
        <value>node0:9000</value>   
    </property>   
    <!-- namenode1的http通信地址 -->   
    <property>   
        <name>dfs.namenode.http-address.node0.namenode1</name>   
        <value>node0:50070</value>   
    </property>   
    <!-- namenode2的RPC通信地址,这里端口要和core-site.xml中fs.defaultFS保持一致 -->   
    <property>   
        <name>dfs.namenode.rpc-address.node0.namenode2</name>   
        <value>node1:9000</value>   
    </property>   
    <!-- namenode2的http通信地址 -->   
    <property>   
        <name>dfs.namenode.http-address.node0.namenode2</name>   
        <value>node1:50070</value>   
    </property>   
  
    <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->   
    <property>   
        <name>dfs.namenode.shared.edits.dir</name>   
        <value>qjournal://node0:8485;node1:8485</value>   
    </property>   
  
    <!-- 配置失败自动切换实现方式 -->   
    <property>   
        <name>dfs.client.failover.proxy.provider.by-j</name>  
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>  
    </property>   
  
    <!-- 配置隔离机制,主要用户远程管理监听其他机器相关服务 -->   
    <property>   
        <name>dfs.ha.fencing.methods</name>   
        <value>sshfence</value>   
    </property>   
    <!-- 使用隔离机制时需要ssh免密码登陆 -->   
    <property>   
        <name>dfs.ha.fencing.ssh.private-key-files</name>   
        <value>/root/.ssh/id_rsa</value>   
    </property>   
  
    <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->   
    <property>   
        <name>dfs.journalnode.edits.dir</name>   
        <value>/usr/local/hadoop/journal</value>   
    </property>   
  
    <!--指定支持高可用自动切换机制-->   
    <property>   
        <name>dfs.ha.automatic-failover.enabled</name>   
        <value>true</value>   
    </property>   
 <property>   
        <name>ha.zookeeper.quorum</name>   
        <value>node0:2181,node1:2181,node2:2181,node3:2181</value>   
    </property>   
  
      
    <property>  
        <name>dfs.namenode.handler.count</name>  
        <value>600</value>  
        <description>The number of server threads for the namenode.</description>  
    </property>  
    <property>  
        <name>dfs.datanode.handler.count</name>  
        <value>600</value>  
        <description>The number of server threads for the datanode.</description>  
    </property>  
    <property>  
        <name>dfs.client.socket-timeout</name>  
        <value>600000</value>  
    </property>  
    <property>    
        <!--这里设置Hadoop允许打开最大文件数，默认4096，不设置的话会提示xcievers exceeded错误-->    
        <name>dfs.datanode.max.transfer.threads</name>    
        <value>409600</value>    
    </property>  
</configuration>
